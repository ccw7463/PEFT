# PEFT(Parameter-Efficient Fine-Tuning)
This is a repository for organizing my studies on PEFT
π’΅
μ „μ²΄ νλΌλ―Έν„°λ¥Ό Fine-Tuning ν•λ”κ² μ•„λ‹ ν•„μ”ν• νλΌλ―Έν„°λ§ μ¬ν•™μµν•λ” λΉ„μ©ν¨μ¨μ  λ°©λ²•
1. PEFT νΉμ§•
π’΅
PEFT μ΄ν•΄
λ€κ·λ¨ λ¨λΈμ νλΌλ―Έν„°λ¥Ό λ€λ¶€λ¶„ κ³ μ •ν• μ±„λ΅ μΌλ¶€ νλΌλ―Έν„°λ§μ„ ν•™μµν•λ‹¤. μΌλ¶€λ§ μ¬ν•™μµν•λ―€λ΅ GPU λ©”λ¨λ¦¬ μ”κµ¬λ‰μ΄ λ‚®μΌλ©°, Foundation Modelμ μ–Έμ–΄λ¥λ ¥κ³Ό μ‚¬μ „ν•™μµμ— μ‚¬μ©λ λ°μ΄ν„°μ— λ€ν• μ •λ³΄κ°€ λ€λ¶€λ¶„ λ³΄μ΅΄λλ‹¤λ” μ¥μ μ΄ μμ.



PEFT μ¥μ 

λ©”λ¨λ¦¬ ν¨μ¨μ„±
μ €μ¥κ³µκ°„ μµμ†ν™” (PEFTλ¥Ό ν†µν•΄ ν•™μµλ λ¨λΈ λ²„μ „μ€ ν¬κΈ°κ°€ μ‘μ)
μ μ‘μ„±κ³Ό μ μ—°μ„±μ΄ μΆ‹μ (λ‹¤μ–‘ν• Taskμ— μ μ©ν•κΈ° μ©μ΄ν•¨)
μ) κ°κΈ° λ‹¤λ¥Έ μ‚¬μ©μμ—κ² λ§μ¶¤ν™”λ μ±—λ΄‡ μ μ‘μ‹ μ†μμ νλΌλ―Έν„°λ§ μ΅°μ •
μ¬λ‚μ  λ§κ°(catastrophic forgetting) λ°©μ§€
catastrophic forgetting μ΄λ€ μƒλ΅μ΄ λ°μ΄ν„°λ¥Ό ν•™μµν•λ©΄μ„ μ΄μ „μ— ν•™μµν–λ μ •λ³΄λ¥Ό λ€λ‰μΌλ΅ μƒλ”κ²ƒμ„ μλ―Έν•¨


2. PEFT μΆ…λ¥μ™€ νΉμ§•
PEFT κΈ°λ²•μ—λ” ν¬κ² Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) κ°€ μμ. μ΄λ¦„μ—μ„λ„ μ• μ μλ“―μ΄, QLoRAμ κ²½μ° LoRA λ°©μ‹μ„ Quantization ν• λ°©μ‹μ΄λ‹¤.



Low-Rank Adaptation (LoRA) 

π’΅
κΈ°μ΅΄μ— ν•™μµν•΄μ•Ό ν•λ” νλΌλ―Έν„°μ™€ λ™μΌν• Shapeλ¥Ό κ°–λ„λ΅ μ‘μ€ ν–‰λ ¬ 2κ°λ¥Ό μ¶”κ°€ν•λ” λ°©μ‹


Vanilla Transformer λ¥Ό μλ΅ μ΄ν•΄ν•κΈ°

Transformer Layer 1κ°μ κ²½μ° 512 x 64 (32,768κ°)μ ν•™μµ νλΌλ―Έν„°λ¥Ό κ°€μ§. μ—¬κΈ°μ— μ‘μ€ ν–‰λ ¬ A, Bλ¥Ό μ¶”κ°€ν•λ‹¤. (ν–‰λ ¬κ³± κ²°κ³Όκ°€ λ™μΌν• shapeλ¥Ό κ°€μ§€λ„λ΅)


μ•„λμ—μ„ β€4β€™λ¥Ό RankλΌ μΉ­ν•¨.
A ν¬κΈ° : 512 x 4  
B ν¬κΈ° : 4 x 64 
Rank κ°’μ€ 4~16 μ—μ„ μΆ‹μ€ μ„±λ¥μ„ λ³΄μ€μ. (μ : 4,8,16)


λ”°λΌμ„, Fine-Tuning μ‹

κΈ°μ΅΄ 512 x 64 shapeλ¥Ό κ°€μ§€λ” νλΌλ―Έν„° κ³ μ • 
A, B ν–‰λ ¬μ„ ν•™μµ β†’ (2048+256)κ°μ νλΌλ―Έν„°λ§ ν•™μµν•λ©΄ λ¨.